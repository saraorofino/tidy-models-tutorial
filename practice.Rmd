---
title: "tidymodels_practice"
output: html_document
---

Following along from the tidymodels tutorial found [here](https://www.tidymodels.org/start/models/).  

```{r setup}
# Load packages
library(tidymodels)

# Helper packages - part 1
library(readr)
library(rstanarm)
library(broom.mixed)
library(dotwhisker)

# Helper packages - part 2
library(nycflights13)
library(skimr)

# Load urchins dataset-adjust according to the tutorial part 1
urchins <- read_csv("https://tidymodels.org/start/models/urchins.csv") %>% 
  setNames(c("food_regime", "initial_volume", "width")) %>% 
  mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High")))
```

# Part 1 - Build and Fit Model

Plot the data: 

```{r}
ggplot(urchins,
       aes(x = initial_volume, 
           y = width, 
           group = food_regime, 
           col = food_regime)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7) + 
  theme_bw()
```


## Basic Model 

Regression model to predict width based on intial volume and feeding regime. In tidymodels use the `parsnip` package, linear regressions use the `linear_reg()` function and then you set the specific engine, or method for training the model, using the `set_engine()` call. The `fit()` function estimates or trains the model. Many models have `tidy()` call that provides summary results in useful formats (dataframe with standard column names):          

```{r}
# Specify the model using the linear_reg() call and then specify the engine using set_engine()
# lm - linear model, ordinary least squares
lm_model <- linear_reg() %>% 
  set_engine("lm")

# Fit the model using the functional form 
lm_fit <- lm_model %>% 
  fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit

# Generate dot-and-whisker plot 
tidy(lm_fit) %>% 
  dwplot(dot_args = list(size=2, color='black'),
         whisker_args = list(color = 'black'),
         vline = geom_vline(xintercept = 0, color = 'grey', linetype = 2))
```

Arguments for dwplot - `ci` confidence interval; if you use `dwplot(list())` you can have multiple model lines show up at one time  

## Making Predictions

Predict the mean body size of urchins with an initial volume of 20 ml at the three different food regimes, using the `predict()` function and the linear model we created     

```{r}
# Generate the new data 
new_points <- expand.grid(initial_volume = 20,
                          food_regime = c("Initial", "Low", "High"))

# Predict mean body weight - output is a tibble
mean_pred <- predict(lm_fit, 
                     new_data = new_points)

# Predict the confidence internval
ci_pred <- predict(lm_fit,
                   new_data = new_points,
                   type = "conf_int")

# Combine the new means and ci 
plot_data <- new_points %>% 
  bind_cols(mean_pred) %>% 
  bind_cols(ci_pred)

# Plot 
ggplot(plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "urchin size") + 
  theme_bw()
```

## New Model 

Bayesian analysis - requires prior distribution for each model parameter. Choosing a wide, bell-shaped Cauchy distribution. Function arguments requires `prior` and `prior_intercept` arguments and there is a `stan` engine that can be callled using `parsnip::set_engine()`  

```{r}
# set the prior distribution
prior_dist <- rstanarm::student_t(df = 1)

# Make the parsnip model 
bayes_mod <- linear_reg() %>% 
  set_engine("stan", 
             prior_intercept = prior_dist, 
             prior = prior_dist) 

# Train the model - specify the functional form
bayes_fit <- bayes_mod %>% 
  fit(width ~ initial_volume * food_regime, data = urchins)

# Use tidy function to update the parameter table
tidy(bayes_fit, conf.int = TRUE)

# Predict the means and confidence interval with the new data
bayes_plot_data <- new_points %>% 
  bind_cols(predict(bayes_fit, new_data = new_points)) %>% 
  bind_cols(predict(bayes_fit, new_data = new_points, type = "conf_int"))

# Plot 
ggplot(bayes_plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) + 
  labs(y = "urchin size") + 
  ggtitle("Bayesian model with t(1) prior distribution") + 
  theme_bw()
```

# Part 2 - Preprocessing Data

Preprocessing with `recipes` package. Recipes are built as a series of preprocessing steps, such as:   

 - converting qualitative predictors to indicator variables (dummy variables)  
 - transforming data to be on a different scale (e.g. log transformation)  
 - transforming whole group predictors together  
 - extracting key features from raw variables (e.g. day of the week from a date)   
 
Using the NYC flights data from `nycflights13`. 

```{r}
# Setup the variables for the example 
flight_data <- flights %>% 
  mutate(
    # Convert the arrival delay to a factor
    arr_delay = ifelse(arr_delay >= 30, "late", "on_time"),
    arr_delay = factor(arr_delay),
    # We will use the date (not date-time) in the recipe below
    date = as.Date(time_hour)
  ) %>% 
  # Include the weather data
  inner_join(weather, by = c("origin", "time_hour")) %>% 
  # Only retain the specific columns we will use
  select(dep_time, flight, origin, dest, air_time, distance, 
         carrier, date, arr_delay, time_hour) %>% 
  # Exclude missing data
  na.omit() %>% 
  # For creating models, it is better to have qualitative columns
  # encoded as factors (instead of character strings)
  mutate_if(is.character, as.factor)

# Glimpse the data using skimr 
flight_data %>% 
  skim(dest, carrier)
```

## Data Splitting 

Split the data into testing and training data (randomly selected). Use a 75/25 split.  

```{r}
# Put 3/4 of the data into the training set 
data_split <- initial_split(flight_data, prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

## Create Recipe and Roles

The `recipe` function has two arguments:  

 - formula: any variable on the left-hand side of the `~` is a model outcome and the right-hand side are predictors   
 - data: recipe is associated with the dataset (the training data) so `data = train_data`  

Add `roles` to the recipe using `update_role()`. We want to retain the `flight` and `time_hour` fields as identifiers but not predictors adn we can reassign their roles using this function. 

```{r}
# Change the flight and time hour to an ID role
flights_rec <- recipe(arr_delay ~ ., data = train_data) %>% 
  update_role(flight, time_hour, new_role = "ID") 

# Current set of variables and roles using the summary function 
summary(flights_rec)
```

Add to the recipe using:   

 - `step_date()`- create two new factor columns with the appropriate day of the week and month   
 - `step_holiday()` - create binary variable indicating whether the current date is a holiday or not   
 - `step_rm()`- remove the original `date` variable since we don't need it anymore  
 - `step_dummy()` - create dummy variables; using `all_nominal()` selects all variables that are either factors or characters; using `-all_outcomes()` removes any outcome variables from this recipe step  
    - Used together this creates dummy variables for all of the factor or character columns unless they are outcomes  
 - `step_zv()` - removes columns from the data when the training set hae a single value so it's added to the recipe after `step_dummy()`  

```{r}
flights_rec <- recipe(arr_delay ~ ., data = train_data) %>% 
  update_role(flight, time_hour, new_role = "ID") %>% 
  step_date(date, features = c("dow", "month")) %>%               
  step_holiday(date, holidays = timeDate::listHolidays("US")) %>% 
  step_rm(date)  %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors())
```

## Fit a Model with a Recipe

Steps:  

 - Build the model 
 - Process the recipe using the training set: involves any estimation or calculations based on the training set; in this case the training set is used to determine which predictors should be converted to dummy variables and which will have zero-variance in the training set   
 - Apply the recipe to the training set: create the final predictor set on the training set   
 - Apply the recipe to the test set: create the final predictor set on the test set  
 
Using the `workflows` package from tidymodels simplifies this process by pairing the model and recipe together. 

```{r}
# Specify the model 
lr_mod <- logistic_reg() %>% 
  set_engine("glm")

# Bundle the parsnip model (lr_mod) with the recipe (flights_rec)
flights_wflow <- workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(flights_rec)

# Fit the model - this object has the finalized recipe and fitted model objects 
flights_fit <- flights_wflow %>% 
  fit(data = train_data)

# For example - pull the fitted model object 
flights_fit %>% 
  pull_workflow_fit() %>% 
  tidy()
```

## Predict 

Used the trained workflow (`flights_fit`) to predict the unseen test data 

```{r}
# Predict a class; late vs. on-time
predict(flights_fit, test_data)

# Predict a probability instead using type = 'prob'
# Bind the output with some of the variables from test data 
flights_pred <- predict(flights_fit, test_data, type = "prob") %>% 
  bind_cols(test_data %>% select(arr_delay, time_hour, flight))
```

## Evaluate Workflow Performance

Evaluate how well the model predicted late arrivals compared tothe true status of our outcome variable `arr_delay`. Try using the area under the curve `roc_curve()` and `roc_auc` from the `yardstick` package  

```{r}
# Graph the curve
flights_pred %>% 
  roc_curve(truth = arr_delay, .pred_late) %>% 
  autoplot()

# Calculate area under the curve 
flights_pred %>% 
  roc_auc(truth = arr_delay, .pred_late)
```

# Part 3 - Evaluate the Model with Resampling

How to characterize the model performance based on resampling statistics  

